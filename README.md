# Machine learning

## Project 2: Machine Learning for Science

In this project, we use [AtLoc](https://github.com/BingCS/AtLoc) to predict positions and orientations for datasets provided by the TOPO lab.

### Step-by-step

- [x] Try to run the code on GitHub
- [x] Plug the TOPO dataset into AtLoc
- [x] Get first results
- [x] Investigate features for best predictions
- [x] Optimize results

### Submission

* **Code**: batch scripts running Python with `torch` ([Scripts](./scripts)) and Python code to adapt AtLoc ([Dataloader](AtLoc-master/data/dataloaders.py), as well as [Run, Train and Eval](AtLoc-master/))
* **Report**: maximum 4 page PDF ([Overleaf Report Draft](https://www.overleaf.com/5419823158fvrtbssxbvwf))

We don't know exactly how the grading will happen... Apparently, it is split evenly between the technical and the coding components.

### Running Code on IZAR

To facilitate reproducibility, here are a few resources to recreate the same environment as we used:

* **Virtual Environment**: [this tutorial](./venvs.md) explains how to set up the `atloc` virtualenv
* **Data**: [this script](scripts/prepare_air) is meant to copy the real and synthetic images of the comballaz dataset with air scene to the right locations (to be run as follows: `scripts/prepare_air`)
* **Train**: [this script](scripts/train_air) trains the model on the comballaz dataset with air scene and puts the resuling weights to `AtLoc-master/logs` (to be run on GPUs *via* `sbatch scripts/train_air --wait`)
* **Test**: [this script](scripts/eval_air) evaluates the previously established model and outputs saliency maps (to be run the same way as `train_air`, with `sbatch scripts/eval_air --wait`)

The `prepare_air` script has not been tested (yet), mainly because I had already run those copy/unzip commands before and didn't need to repeat them.

#### Naming
The root of this project should be called `cs433-atloc4topo` for the scripts to work (instead of `cs-433-project-2-eightyears`).

#### Dependencies
The dependencies for Python are listed in [this tutorial](./venvs.md), along with the virtual environment setup guide.


### Obtaining the Data
Data has been kindly provided to us by the Topo lab at EPFL. It can be accessed through IZAR, where it is located in the directories indicated in the scripts folder: `/work/topo/VNAV`.

Some sample data is available at [https://drive.google.com/drive/folders/1MKgI7ejn7j6Z5nbwau4jFjYrCiKukupx?usp=sharing](https://drive.google.com/drive/folders/1MKgI7ejn7j6Z5nbwau4jFjYrCiKukupx?usp=sharing) without requesting access to IZAR folders.


### Running Code Locally
We do not recommend running code locally as it training (100 epochs) can take up to days. However, evaluating model accuracy and producing saliency maps can be done locally within reasonable time frame.
For simplicity, we have provided some weights on google drive `Link followows shortly`. Like in the original AtLoc paper, data should be located in `./AtLoc-master/data` folder with the according hierarchies (*e.g.* `/comballaz/air/`).

#### Stats File
This file is needed for AtLoc eval functions to work and can be generated by:

```bash
python run.py --dataset comballaz --scene air
```

It is also done automatically at the beginning of the scripts we provide.

#### Evaluate Weights
This file is needed for AtLoc eval functions to work and can be generated by:

```bash
python eval.py --dataset comballaz --scene air --weights ./logs/comballaz_air_AtLoc_False/models/epoch_000.pth.tar
```

This example would evaluate the air dataset (located in `data/comballaz/air`) on the weight located at the given path

#### Saliency Maps
Our saliency maps show the difference AtLoc weights provides compared to pre-trained weights. To produce the maps, you will need to specify the weight to compare, for example, for the air dataset:

```bash
python run.py --dataset comballaz --scene air --weights ./logs/comballaz_air_AtLoc_False/models/epoch_000.pth.tar --final_weights ./logs/comballaz_air_AtLoc_False/models/epoch_095.pth.tar
```

compares the comballaz air dataset (located in `data/comballaz/air`) on the weights located at the given paths. If the `--final_weights` option is missing, run will simply produce the stats file.
Saliency maps are stored in the `/figures` directory in the according logs models folder.


### Known Limitations

While AtLoc is trained and tested on Python 2 (miniconda 2), code on IZAR should be run on Python 3 (with `pip`) due to compatibility issues with modules. 
Depending on the Python version used in training, the pickle module is unable to load weights. This implies that training and evaluating should be done in the same environment.



